import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error

# Load and prepare data
data = pd.read_csv("C:/Users/Pranav/Downloads/BostonHousing.csv")
X = data.drop(columns=['medv']).values
Y = data['medv'].values
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Track errors for different model complexities
train_errors, test_errors = [], []

for d in range(1, 10):
    poly = PolynomialFeatures(degree=d)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)
    
    model = Ridge(alpha=1.0)
    model.fit(X_train_poly, Y_train)
    
    train_errors.append(mean_squared_error(Y_train, model.predict(X_train_poly)))
    test_errors.append(mean_squared_error(Y_test, model.predict(X_test_poly)))

# Plot
plt.plot(range(1, 10), train_errors, label='Train Error', marker='o')
plt.plot(range(1, 10), test_errors, label='Test Error', marker='o')
plt.xlabel("Polynomial Degree")
plt.ylabel("MSE")
plt.title("Overfitting vs Underfitting")
plt.legend()
plt.grid(True)
plt.show()
